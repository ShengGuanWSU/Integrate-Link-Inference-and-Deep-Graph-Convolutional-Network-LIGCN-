%\vspace{-1ex}
%%%%%%%%%%%%%%%%%%% Section 8 %%%%%%%%%%%%%%%%%%%%%%%
\section{Experiment 2-- LIGCN}
\label{sec-expt}

Using real-world and synthetic dataset, we experimentally verify the efficiency and effectiveness of our LIGCN techniques.

\stitle{Datasets} We used one real-world graphs, that is \textbf{{\em DBpedia}} \footnote{\scriptsize\url{https://wiki.dbpedia.org/develop/datasets}}, a knowledge-based graph that contains a set of triples with size of $33,449,633$. We used a parser to further process this giant knowledge graph to a graph with $9595$ nodes and $5540$ edges.
Each node includes $4$ attributes and each edge reflects the ``influence" relationship.

We also employed \textbf{{\em BSBM}} \footnote{\scriptsize\url{http://wifo5-03.informatik.uni-mannheim.de/bizer/berlinsparqlbenchmark/}}
e-commerce benchmark to generate synthetic knowledge graphs over a set of products with different
types, related vendors and offers made by vendors. The generator is
controlled by the number of nodes (up to 60M), edges (up to
152M), and attribute labels drawn from an alphabet $\Sigma$ of $3080$ labels. We processed these synthetic knowledge graphs further to aggregate related attributes and generated a graph with two types of entities- product and offer. Different offers that are made by the same vendor are connected by edges and different offers for the same product are also connected. 

As there is no ground truth of erroneous entities in the above datasets, we need to inject errors into the attributed networks for our empirical evaluation. We refer to error generation tasks in {\em BART} \cite{arocena2015messing} to control the error-generation process. {\em BART} can generate constraint-induced errors with a set of configuration parameters , e.g. error percentage for each violation of the constraints in a systematic manner. At the same time, {\em BART} is capable to generate random errors of several kinds, e.g. typos, bogus or null values, and outliers. For instance, in the synthetic dataset, we organized product and offer entities respectively as two relation tables and manually construct a set of functional dependencies or conditional functional dependencies,  e.g. for products with the same product id in two tables, their product type should be the same. At the same time, we added some random errors into these two tables. Then, by comparing the difference between the clean and polluted tables and transferring them back to the graph organization, we got the ground truth labels.

\stitle{Compared Methods}. We compare the proposed \textbf {LIGCN} framework with the following popular and state-of-the-art erroneous entity detection methods:
(1) GCN \cite{kipf2016semi} jointly considers the node attributes and graph topology in the semi-supervised setting.
(2) GraphSAGE \cite{hamilton2017inductive} jointly considers the node attributes and graph topology but needs the supervised setting. In the experiment, we remove all the nodes without labels and their affiliated edges from the graph. We can treat GraphSAGE as a special form of GCN that implements node-wise sampling to get a fixed number of neighbors and also has the flexibility to select the aggregation function for node embedding.

\stitle{Metrics}
In this experiment, we evaluate the percentage of the edges we removed from the \textbf{{\em DBpedia}} graph and how the link prediction model can recover the incomplete graph and construct some ``hidden links" that can enhance the performance of the \textbf{LIGCN} framework.
Table 1-5 shows that \textbf{LIGCN} indeed can enhance the performance of corresponding model GCN or GraphSAGE by adopting the same aggregation function they use without sacrificing the accuracy performance. 

\begin{table}[]
\begin{adjustbox}{width=\columnwidth,center}

\begin{tabular}{|l|l|l|l|l|l|l|l|l|}
\hline
 Removing Percentage&  0.1&0.2  &0.3  &0.4  &0.5  &0.6  &0.7  &0.8   \\ \hline
 GraphSAGE & 0.61& 0.66&	0.71&	0.63&	0.60&	0.56&	0.56&	0.47 \\ \hline
 LIGCN & 0.67&	0.71&	0.77&	0.71&	0.64&	0.64&	0.58&	0.49 \\ \hline
 Gain (\%)&9.98&	6.36&	8.26&	12.26&	5.69&	12.86&	2.75&	2.88 \\ \hline
\end{tabular}


\end{adjustbox}
\caption{Precision:LIGCN vs GraphSage}
\label{tab-compare1} 
\end{table}

\begin{table}[]
\begin{adjustbox}{width=\columnwidth,center}

\begin{tabular}{|l|l|l|l|l|l|l|l|l|}
\hline
 Removing Percentage&  0.1&0.2  &0.3  &0.4  &0.5  &0.6  &0.7  &0.8   \\ \hline
 GraphSAGE & 0.72&	0.76&	0.78&	0.73&	0.71&	0.69&	0.68&	0.63	 \\ \hline
LIGCN & 0.76&	0.77&	0.80&	0.77&	0.73&	0.73&	0.70&	0.63 \\ \hline
 Gain (\%)&5.00&	1.96&	2.01&	4.84&	1.87&	7.02&	2.46&	0.17	\\ \hline
\end{tabular}


\end{adjustbox}
\caption{F1-Score:LIGCN vs GraphSage}
\label{tab-compare2} 
\end{table}

\begin{table}[]
\begin{adjustbox}{width=\columnwidth,center}

\begin{tabular}{|l|l|l|l|l|l|l|l|l|}
\hline
 Removing Percentage&  0.1&0.2  &0.3  &0.4  &0.5  &0.6  &0.7  &0.8   \\ \hline
 GraphSAGE & 0.79	&0.83&	0.85&	0.8	&0.78&	0.75&	0.75&	0.66	 \\ \hline
LIGCN & 0.83&	0.85&	0.87&	0.84&	0.8&	0.81&	0.76&	0.68	 \\ \hline
Difference&	+0.04&	+0.02&	+0.02&	+0.04&	+0.02&	+0.06&	+0.01&	+0.02		\\ \hline
\end{tabular}


\end{adjustbox}
\caption{Accuracy:LIGCN vs GraphSage}
\label{tab-compare3} 
\end{table}


\begin{table}[]
\begin{adjustbox}{width=\columnwidth,center}

\begin{tabular}{|l|l|l|l|l|l|l|l|l|}
\hline
 Removing Percentage&  0.1&0.2  &0.3  &0.4  &0.5  &0.6  &0.7  &0.8   \\ \hline
GCN & 0.42&	0.45&	0.43&	0.41&	0.49&	0.48&	0.47&	0.46		 \\ \hline
LIGCN &0.48&	0.49&	0.48&	0.47&	0.49&	0.50&	0.49&	0.47		 \\ \hline
Gain(\%)&13.85&	7.23&	10.86&	13.85&	0.26&	4.40&	4.51&	1.95		\\ \hline
\end{tabular}


\end{adjustbox}
\caption{F1-Score:LIGCN vs GCN}
\label{tab-compare4} 
\end{table}


\begin{table}[]
\begin{adjustbox}{width=\columnwidth,center}

\begin{tabular}{|l|l|l|l|l|l|l|l|l|}
\hline
Removing Percentage&  0.1&0.2  &0.3  &0.4  &0.5  &0.6  &0.7  &0.8   \\ \hline
GCN & 0.55&	0.54&	0.56&	0.57&	0.52&	0.55&	0.52&	0.50		 \\ \hline
LIGCN & 0.54&	0.53&	0.56&	0.59&	0.54&	0.57&	0.52&	0.52		 \\ \hline
Difference&	-0.013&	-0.003&	-0.004&	0.011&	0.015&	0.02& 0&0.01321			\\ \hline
\end{tabular}


\end{adjustbox}
\caption{Accuracy: LIGCN vs GCN}
\label{tab-compare3} 
\end{table}




Similarly, we also create synthetic graph \textbf{{\em BSBM}} and evaluate the accuracy comparison between GCN and \textbf{LIGCN}. Base on our observation \textbf{LIGCN} achieves better accuracy than GCN. \textbf{LIGCN} outperforms GCN by 4\%, it improves the accuracy from 78\% to 81.3 \%.


