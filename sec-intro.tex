

%%%%%%%%%%%%%%%%%%% Section 8 %%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec-intro}

Deep learning has achieved huge success on many machine learning tasks, ranging from image classification, video processing, speech recognition to natural language processing. The success of deep learning is partially attributed to the advantage that deep learning methods can effectively extract latent representations from data in Euclidean space. For example, image data can be represented as a regular grid in the Euclidean space.

Meanwhile, there is an increasing number of applications where data are naturally organized in the form of graphs. For instance, in citation network, papers can be represented as nodes and their citationships can be treated as edges. All the nodes in the graph need to be classified into different domains, such as data mining, artificial intelligence, and so on. 

Recently, there are lots of studies related to learning on graph structural data. Graph neural networks (GNNs) \cite{gori2005new} have been proposed to extend deep learning approaches for graph data. GNNs take the topological structure and nodal attributes as input and design their encoder functions carefully that rely on a node's local 
neighborhood, but not necessarily the entire graph. We call these approaches are neighborhood aggregation based methods. In the GNNs, we have a matrix of node features, e.g., categorical attributes, text, node degrees, and so on. The neighborhood aggregation methods leverage this attribute information to inform their embeddings. In such a way, GNNs can learn discriminative node embeddings for outlier detection tasks and have already achieved huge success \cite{ding2019deep,zhong2019graph}. Among GNNs, the most representative method is convolutional Graph Neural Networks (ConvGNNs) \cite{wu2019comprehensive}. ConvGNNs generalizes the operation of convolution from grid data to graph data. ConvGNNs is able to learn node embeddings by stacking multiple graph convolutional layers and plays an important role in building up many other complex models. Graph Convolutional Networks (GCN) \cite{kipf2016semi} is one of most representative ConvGNNs models. In particular, GCN employed a first-order approximation of spectral graph convolution and achieved huge success. 

GCN can be used to process the data of Non Euclidean Structure. The academic expression is that it can maintain translation invariance on the data of Non Euclidean Structure. Since we hope to effectively extract spatial features for machine learning on such a data structure (topological graph), GCN has become the focus of research. In a broad sense, any data can establish topological association in the normed space. Spectral clustering is the application of this idea (summary of spectral clustering principle). So topological connection is a generalized data structure, and GCN has a lot of application space \cite{qimaiinsight2018}. GCN is a natural extension of the convolutional neural network on the graph domain. It can perform end-to-end learning on node feature information and structure information at the same time. It is a good choice for graph data learning tasks. Graph convolution has a wide applicability and is applicable to nodes and graphs of any topology. For tasks such as node classification and edge prediction.

 However, it is well-studied that GCN is limited to have a shallow structure. Stacking multiple GCN layers may oversmooth features of nodes from different clusters and reduce its discriminative power of node embedding \cite{sun2019adagcn}. This significantly limits all the models based on GCN architecture that their models' effective receptive fields are limited to the local neighborhood and cannot capture long-range dependencies of the graph. However, the ability to capture non-local dependency is one key for deep neural networks as argued in \cite{wang2018non}. Some very recent work proposed several methodologies to alleviate this oversmoothing issue either by carefully selecting and combining different representation learned from different distances, not necessary receiving latent representations starting from their immediate (first-hop) neighbors and from further N-hop neighbors at every message passing step \cite{kapoor2019mixhop,xu2018representation} or leveraging personalized page rank to derive an improved propagation scheme \cite{klicpera2018predict}.
 
 What is worse, all GNNs fail to take erroneous information contained in the graph into consideration. They assumed that the given graph is complete (no missing edges) and error free, which is not the usual case for the real-world graph. 
 
 Motivated by observations like the above, in this paper, we address two questions. First, we 
 admit that the node $v$ and its local neighborhood in the graph may contain the erroneous information. At the same time, the given graph may contain missing edges that hurt the effectiveness of neighborhood aggregation. In this case, we propose an architecture that, as opposed to existing models, enables better structure-aware representation learning by considering long-range dependencies learning. Secondly, we show the effectiveness of the proposed architecture. Combining the link prediction architecture with models like GCN, GraphSAGE \cite{hamilton2017inductive} consistently improves those models' performance.
 
 \stitle{Contributions}. 
 (1) We implement a system to recommend files(e.g., books and papers) to related person in order to test the performance of GCN. To make recommendation, the file and person relationship can be viewed as a bipartite graph in which every node pair can only have connection from different entity, inspired by a recent paper \cite{pintest}. Based on such structure of input, recommendations can be made by training the GCN.
 
 \stab
 (2) We propose a novel hidden link inference architecture that flexibly leverages long-range dependencies of each node as a necessary supplement along with its local neighborhood for neighborhood aggregation procedure. In this way, we can better capture the structure-aware representations that can be applied in the latter tasks.
 
 \stab
 (3) We admit that the real-world graphs can be incomplete, e.g. containing missing edges and erroneous where nodes can contain errors, e.g. dirty attributes, missing attributes. 
 To show the effectiveness of hidden link inference architecture, we combine this architecture with models like GCN, GraphSAGE and test the whole framework on the task of identifying nodes that contain erroneous information. We call this task as Erroneous Entity Detection task. 
 We transform Erroneous Entity Detection task to a classic classification task in the semi-supervised setting by obtaining limited labels either from rule-based models or from manual annotation.
 
 \stab
 (4) We evaluate the performance of the framework on synthetic datasets and real-world datasets. It is non-trivial to process and obtain datasets for Erroneous Entity Detection task in the graph setting. To better model the real-world errors, we adopted BSBM benchmark as synthetic datasets generator and Bart benchmark as error generation tool. More details will be discussed in Experiment section.







\stitle{Related Work.} We categorize the related work as follows.

\etitle{Graph Convolutional Networks.}
Among all the ConvGNNs, the most representative method is graph convolutional network (GCN). GCN is a special form of Laplacian smoothing \cite{li2018deeper} and is able to learn node embeddings by stacking multiple graph convolutional layers and achieves the state-of-the-art performance in the semi-supervised node classification task. Lots of following work are built upon GCN to address its limitations and expand its usage for other tasks. GraphSAGE \cite{hamilton2017inductive} took one step further compared to GCN that it concatenates self embedding and neighbor embedding together and permits general aggregation functions over sampled fixed-size neighborhood of each node. The more complex aggregators on certain tasks gives significant gains. However, the limitation of GraphSAGE is that it requires a fully supervised setting and makes it less attractive given the fact that it is labor-intensive to collect labels for real-life tasks. GAN \cite{velivckovic2017graph} integrated self-attention mechanism to learn a weight coefficient for each neighbor of each point in order to average features from neighborhood of nodes. Rex Ying et al, develop a data-efficient GCN algorithm PinSage which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information \cite{pintest}. Pinsage is a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model.  

GCN, GraphSAGE, and GAN all can be categorized as ConvGNN methods \cite{wu2019comprehensive}, where each nodeâ€™s hidden representation is encapsulated by aggregating
feature information from its neighbors. By stacking multiple but limited layers, the final hidden representation of each node receives messages from its local neighborhood. 

\etitle{Rule-based Model for Erroneous Entity Detection
}. Traditionally, functional dependency \cite{kolahi2009approximating} and conditional functional dependency \cite{bohannon2007conditional} have achieved huge success to identify errors when data is organized into relations with a fixed set of attributes. Functional dependency can be treated as a set of rules. People can easily find violations among different relation tables and locate erroneous data based on these rules. As a large amount of data is converted into RDF format by a variety of tools, the graph representation becomes popular and well adopted to support emerging applications such as knowledge search \cite{jindal2014review},  fact checking \cite{fionda2018fact}, and recommendation \cite{zhang2016collaborative}. In the RDF format, a graph consists of a set of triples $<v_x$, $r$, $v_y>$, where $v_x$ and $v_y$ denote a subject entity and an object entity, respectively, and $r$ refers to a predicate (a relationship) between $v_x$ and $v_y$.
This RDF graph representation at one time still can be viewed as a graph database, therefore we can benefit from traditional table-based approaches. On the other hand, the RDF graph representation requires us to extend the functional dependency in a way that can work on extremely decomposed tables since RDF data is not normally organized into relations with a fixed set of attributes. The graph 
organization provides us a new possibility to apply graph patterns for error detection and extends the traditional rule-based models on tables. \cite{yu2011extending} proposed value-clustered graph functional dependency to extend the functional dependencies (VGFDs) that can 
construct the dependencies across the
whole data set schema instead of a single relation. Moreover, the VGFDs can better model the probabilistic in nature regarding dependencies since one-to-one value correspondences are inappropriate in the RDF graph setting (e.g. the days needed for processing an request is usually limited to a range description but not an exact value).
Functional dependencies for graphs \cite{fan2016functional} have been proposed to capture errors in graph data. 
Compared to VGFDs, graph functional dependencies support general topological constraints
that enforce topological and value constraints by incorporating graph patterns with variables and subgraph isomorphism. Now, the hard constraints (e.g., subgraph isomorphism) are useful for detecting data inconsistencies.  \cite{fan2017dependencies} further proposed a class of dependencies, referred to as graph entity dependencies (GEDs). A GED is a combination of graph pattern and an attribute dependency that can be used to catch inconsistencies.   
Although these Rule-based Models are useful for erroneous entity detection task, they usually assume the graph they relied on for learning the set of the rules is complete that usually is not the case for the real-world graphs. The real-world incomplete graphs will lead to incomplete set of rules that cannot cover all the errors that occurred in the graph data sets. Moreover, for the rule-mining methodologies proposed in rule-based models that are helpful for error detection task, such as AMIE \cite{galarraga2013amie,galarraga2015fast} that discovers rules with conjunctive Horn clauses, GFDs \cite{lin2019discovering} that involves subgraph isomorphism, and
graph fact checking rules (GFCs) that incorporates graph patterns, these methodologies usually need to compute pattern matching and are computationally hard in general. Our approach doesn't need to build rules from scratch or relies on some pre-defined rules for the error detection task. To our best knowledge, this is the first attempt to solve the detection of erroneous entity problem on attributed networks by developing a carefully designed graph neural network
model.

\etitle{Learning-based Model for Erroneous Entity Detection}.
Another type of approach focuses on learning models that can be used to capture nodes' errors \cite{rashid2019completeness}. 
In \cite{krishnan2016activeclean}, the authors proposed a progressive data cleaning framework that supports common convex loss models (e.g., linear regression and SVMs) for error detection and cleaning on traditional table-based records. Authors in \cite{wienand2014detecting} presented a probabilistic framework using the relations (equal, greater than, less than) among multiple
RDF predicates to detect inconsistencies in numerical and date
values based on the statistical distribution of predicates and objects in RDF datasets.
As the data is organized more naturally into graph representations, the model parameter learning in this class starts leveraging more information from the graph representation. The model parameters usually are not only determined by the 
mutual interactions between nodes (e.g.topological structure), but also are determined by their content dissimilarity
(e.g. nodal attributes).  PaTyBRED \cite{melo2017detection} proposed an error detection method which relies on path and type features used by a classifier for
every relation in the graph exploiting local feature selection.  In contrast to this class of work that used linear or shallow model to model the attributed networks, our approach shows the significance of developing a novel deep architecture on attributed networks that can better model the non-linearity between the node interactions and nodal attributes. Besides, our approach also can deal with the situation that the graph has missing edges whereas the previous methods did not consider that at all. 

\etitle{Outlier Detection}.
\eat{Representation Learning on Graphs with Jumping Knowledge Networks
MixHop: Higher-Order Graph Convolutional Architectures
via Sparsified Neighborhood Mixing
}
Although outlier detection does
not necessarily identify errors, but also special and useful information
(e.g., the population of very large cities), it has been
shown that the vast majority of outliers identified are
actual errors \cite{fleischhacker2014detecting}. This finding leads to a fact that the outlier detection problem has a huge overlap of our erroneous entity detection problem. At the same time, the anormaly detection work is shown to be useful in the error detection task. ANOMALOUS \cite{peng2018anomalous} performs joint anomaly detection and attribute selection to detect anomalies on attributed networks based on the CUR decomposition and residual analysis.  In \cite{fleischhacker2014detecting}, authors combined the outcomes of two independent outlier detection runs to get
a more reliable result, as well as to confirm or reject the assessment of an error.  Recently, the node embedding and graph representation learning algorithms introduce the GCN into the outlier detection task and achieves the state-of-the-art performance \cite{ding2019deep,zhong2019graph}. It remains unclear how its power can be shifted to the erroneous entity detection problem. As we know that GCN-based models encounter oversmoothing when we stack many GCN layers in the model, almost all GCN-based models are limited to have shallow model architectures with only two or three layers. Some techniques have been proposed to deal with this oversmoothing bottleneck. 
Our approach also tries to overcome oversmoothing issue by explicitly changing immediate neighbors during the neighborhood aggregation such that each node receives latent representations from its topological first-hop neighbors as well as some non-local representations that are semantically close to itself, e.g. sharing the same label and having higher chance to be visited in the personalized page rank propagation scheme. Our approach doesn't involve fusion procedure as previous approaches \cite{kapoor2019mixhop,xu2018representation} where distance selection is normally hard to interpret and corresponding performance relies heavily on the underlying data. Meanwhile, to our best knowledge, our approach is the first attempt to consider combining graph completion and embedding learning together for erroneous entity detection. All the current GNNs-based deep neural network approaches assume the input graph is complete and error-free, whereas in the real-world application this assumption is barely true. 
\eat{This class of GNN-based graph embedding methods takes the topological structure and nodal attributes as input and designs their encoder functions carefully that rely on a node's local 
neighborhood, but not necessarily the entire graph. We call these approaches are neighborhood aggregation based methods. In the GNNs, we have a matrix of node features, e.g., categorical attributes, text, node degrees, and so on. The neighborhood aggregation methods leverage this attribute information to inform their embeddings. In such a way, GNN-based methods can learn discriminative node embeddings for outlier detection tasks and have already achieved huge success \cite{ding2019deep,zhong2019graph}. Among GNN-based methods, the most representative method is convolutional Graph Neural Networks (ConvGNNs) \cite{wu2019comprehensive}. ConvGNNs generalizes the operation of convolution from grid data to graph data. ConvGNNs is able to learn node embeddings by stacking multiple graph convolutional layers and plays an important role in building up many other complex models. Graph Convolutional Networks (GCN) \cite{kipf2016semi} is one of most representative ConvGNNs models. In particular, GCN employed a first-order approximation of spectral graph convolution and achieved huge success.}\eat{Among GNN-based methods, the most representative method is graph convolutional network (GCN). GCN is a special form of Laplacian smoothing \cite{} and is able to learn node embeddings by stacking multiple graph convolutional layers and achieves the state-of-the-art performance in the semi-supervised node classification task. Lots of following work are built upon GCN to address its limitations and expand its usage for other tasks, e.g. outlier detection \cite{,,,}. GraphSAGE \cite{} took one step further compared to GCN that it concatenates self embedding and neighbor embedding together and permits general aggregation functions. The more complex aggregators on certain tasks gives significant gains. However, the limitation of GraphSAGE is that it requires a fully supervised setting and makes it less attractive given the fact that it is labor-intensive to collect labels for real-life tasks. However, it is well-studied that GCN is limited to have a shallow structure. Stacking multiple GCN layers may oversmooth features of nodes from different clusters and reduce its discriminative power of node embedding \cite{sun2019adagcn}. This significantly limits all the models based on GCN architecture that their models' effective receptive fields are limited to the local neighborhood and cannot capture long-range dependencies of the graph. However, the ability to capture non-local dependency is one key for deep neural networks as argued in \cite{wang2018non}. Some very recent work proposed several methodologies to alleviate this oversmooth issue either by carefully selecting and combining different representation learned from different distances, not necessary receiving latent representations starting from their immediate (first-hop) neighbors and from further N-hop neighbors at every message passing step \cite{kapoor2019mixhop,xu2018representation} or leverages personalized page rank to derive an improved propagation scheme \cite{klicpera2018predict}.} 